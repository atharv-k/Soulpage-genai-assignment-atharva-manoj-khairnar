# Company Intelligence Agentic System (LangGraph)

---

### 1. System Objective

The primary goal of this system is to evaluate the understanding of multi-agent workflows and LangGraph orchestration. It provides a quick, automated intelligence report for any publicly-traded company by:

1.  **Fetching** the latest news and performance data.
2.  **Analyzing** the raw data to extract key insights, risk factors, and overall sentiment.

### 2. Architectural Overview: LangGraph Orchestration

The system employs a simple, linear workflow managed by a **LangGraph State Machine**. The core mechanism for collaboration and context is the shared **AgentState**.

#### 2.1. Shared State Definition (`AgentState`)

The state is a typed dictionary that acts as the memory and data pipeline, passing information between the agents:

| Field | Type | Description |
| :--- | :--- | :--- |
| `company_name` | `str` | The initial input (e.g., "Tesla Inc."). |
| `raw_data` | `str` | Raw data/search results collected by Agent 1. |
| `final_report` | `str` | The structured analysis generated by Agent 2. |
| `messages` | `List[tuple]` | The history of all interactions, crucial for maintaining full context/memory. |

#### 2.2. Workflow Diagram and Agent Roles

The graph defines a two-step process:

```mermaid
graph TD;
    A[Start: User Input (Company Name)] --> |Initial State| B(Node: Data Collector Agent 1);
    B --> |Update State: raw_data| C(Node: Analyst Agent 2);
    C --> |Update State: final_report| D[End: Output Final Report];
```
# Conversational Knowledge Bot (LangChain Agent)

This project implements a conversational knowledge bot using the LangChain framework. It fulfills the core requirements of memory, external tool usage, and contextual responses by leveraging the `initialize_agent`.

## Design Overview

### 1. Agent and LLM
* **Core Logic:** `initialize_agent` is used, utilizing a **ReAct** (Reasoning and Action) prompt template. This allows the LLM to dynamically generate a `Thought`, call the `Action` (tool), process the `Observation` (tool result), and generate a `Final Answer`.
* **LLM:** `ChatGroq` (llama-3.1-8b-instant) is used for its reliable tool-calling and reasoning capabilities.

### 2. Memory Design
* **Component:** `ConversationBufferMemory` is the memory component.
* **Mechanism:** It stores the full, raw text of the conversation history. This history is injected into the agent's prompt via the `{chat_history}` variable *before* the current user input is processed. This enables the bot to resolve co-references (like "he" or "it") and provide contextual follow-up answers.

### 3. Tool Integration
* **Tool:** `DuckDuckGoSearchRun` is wrapped as a LangChain `Tool`.
* **Purpose:** This tool is used for all queries requiring **current and factual** information, solving the LLM's knowledge cutoff limitation. The agent's system prompt explicitly instructs the agent to use this tool when needed.

## Sample Chat Log

**(From running `streamlit run bot.py` and observing the console output)**

| Interaction | Expected Behavior |
| :--- | :--- |
| **Human:** Who is the CEO of Tesla? | **Factual Query:** Agent uses `DuckDuckGoSearch` tool. |
| **AI:** The CEO of Tesla is Elon Musk. | |
| **Human:** Where did he study? | **Contextual Follow-up:** Agent uses memory to link "he" to "Elon Musk" and uses `DuckDuckGoSearch` again. |
| **AI:** Elon Musk studied at Queen's University in Kingston, Ontario, Canada, and later transferred to the University of Pennsylvania, where he graduated with a degree in economics in 1997. | |
| **Human:** Where does he live now? | **Contextual + Factual:** Agent uses memory, formulates a new search query based on history, and uses the tool. |
| **AI:** Elon Musk currently resides in various locations, including Los Angeles, California, and Austin, Texas, in the United States. | |
